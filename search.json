[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pyssed ",
    "section": "",
    "text": "pyssed implements methods for anytime-valid inference on average treatment effects (ATEs) in adaptive experiments. It implements the Mixture Adaptive Design (MAD) (Liang and Bojinov, 2024), which delivers asymptotic confidence sequences for ATEs under arbitrary adaptive designs, along with extensions (Molitor and Gold, 2025) that add covariate-adjusted estimation and dynamic sample balancing to enhance precision and statistical power.\n\n\npyssed can be installed from PyPI with:\npip install pyssed\nor from GitHub with:\npip install git+https://github.com/dmolitor/pyssed\n\n\n\nWe’ll simulate a simple binary experiment (\\(w \\in \\{0, 1\\}\\)) and demonstrate how MAD enables unbiased ATE estimation and how covariate-adjusted MAD (MADCovar) can provide significant improvements in precision.\nIndividual-level potential outcomes are generated as i.i.d draws from \\(Y_i(w) \\sim \\text{Bernoulli}(\\mu_i(w))\\) with \\[\\mu_i(w) = 0.35 + \\mathbf{1}\\{W_i=w\\}0.2 + 0.3 X_{1,i} + 0.1 X_{2,i} - 0.2 X_{3,i},\\] where \\(X_{1,i},X_{2,i},X_{3,i}\\sim\\text{Bernoulli}(0.6), \\text{Bernoulli}(0.5), \\text{ and } \\text{Bernoulli}(0.7)\\), respectively. This results in ATE \\(=0.2\\).\nWe’ll first spin up a reward function that will generate individual-level potential outcomes and covariates drawn from this data-generating process. We will have one reward function that records the covariates and one that ignores the covariates.\n\nimport numpy as np\nimport pandas as pd\nimport plotnine as pn\nfrom pyssed.bandit import Reward, TSBernoulli\nfrom pyssed.model import FastOLSModel\nfrom pyssed import MAD\n\ngenerator = np.random.default_rng(seed=123)\n\ndef reward_covariates(arm: int):\n    ate = {0: 0., 1: 0.2}\n    X1 = np.random.binomial(1, 0.6)\n    X2 = np.random.binomial(1, 0.5)\n    X3 = np.random.binomial(1, 0.7)\n    ate = ate[arm]\n    mean = 0.35 + ate + 0.3 * X1 + 0.1 * X2 - 0.2 * X3\n    Y_i = generator.binomial(n=1, p=mean)\n    X_df = pd.DataFrame({\"X_1\": [X1], \"X_2\": [X2], \"X_3\": [X3]})\n    return Reward(outcome=float(Y_i), covariates=X_df)\n\ndef reward_no_covariates(arm: int):\n    return Reward(outcome=reward_covariates(arm=arm).outcome)\n\nNext, we will run MAD both with and without covariates. We will see that both approaches give us valid inference, but that, unsurprisingly, including covariates gives us improvements in the precision. We set Thompson Sampling as the underlying adaptive assignment algorithm, a quickly-decaying sequence \\(\\delta_t = \\frac{1}{t^{1/0.24}}\\), and (for MADCovar) a linear probability model for the outcome models fit on \\(\\{X_{1, i}, X_{2, i}, X_{3, i}\\}\\). Finally, we simulate MAD and MADCovar for 2,000 units and calculate the corresponding ATE estimates and 95% CSs.\nFirst, we will run our experiment with MADCovar:\n\nmad_covariates = MAD(\n    bandit=TSBernoulli(k=2, control=0, reward=reward_covariates),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=int(2e3),\n    model=FastOLSModel,\n    pooled=True\n)\nmad_covariates.fit(\n    verbose=False,\n    early_stopping=False,\n    mc_adjust=None\n)\n\nand then repeat the experiment with MAD:\n\nmad_no_covariates = MAD(\n    bandit=TSBernoulli(k=2, control=0, reward=reward_no_covariates),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=int(2e3)\n)\nmad_no_covariates.fit(verbose=False, early_stopping=False, mc_adjust=None)\n\nNow, we can compare the precision of the 95% CSs for both methods.\n\n\n\n\n\n\n\n\n\nWe see that, as expected, including covariates improves our precision across the entire sample horizon.\nFor further details and examples, please check out the remainder of our documentation!\n\n\n\n [1] Liang, B. and Bojinov, I. (2024). An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits. arXiv:2311.05794.\n [2] Molitor, D. and Gold, S. (2025). Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment and Balanced Power. arXiv:2506.20523"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "pyssed ",
    "section": "",
    "text": "pyssed can be installed from PyPI with:\npip install pyssed\nor from GitHub with:\npip install git+https://github.com/dmolitor/pyssed"
  },
  {
    "objectID": "index.html#example",
    "href": "index.html#example",
    "title": "pyssed ",
    "section": "",
    "text": "We’ll simulate a simple binary experiment (\\(w \\in \\{0, 1\\}\\)) and demonstrate how MAD enables unbiased ATE estimation and how covariate-adjusted MAD (MADCovar) can provide significant improvements in precision.\nIndividual-level potential outcomes are generated as i.i.d draws from \\(Y_i(w) \\sim \\text{Bernoulli}(\\mu_i(w))\\) with \\[\\mu_i(w) = 0.35 + \\mathbf{1}\\{W_i=w\\}0.2 + 0.3 X_{1,i} + 0.1 X_{2,i} - 0.2 X_{3,i},\\] where \\(X_{1,i},X_{2,i},X_{3,i}\\sim\\text{Bernoulli}(0.6), \\text{Bernoulli}(0.5), \\text{ and } \\text{Bernoulli}(0.7)\\), respectively. This results in ATE \\(=0.2\\).\nWe’ll first spin up a reward function that will generate individual-level potential outcomes and covariates drawn from this data-generating process. We will have one reward function that records the covariates and one that ignores the covariates.\n\nimport numpy as np\nimport pandas as pd\nimport plotnine as pn\nfrom pyssed.bandit import Reward, TSBernoulli\nfrom pyssed.model import FastOLSModel\nfrom pyssed import MAD\n\ngenerator = np.random.default_rng(seed=123)\n\ndef reward_covariates(arm: int):\n    ate = {0: 0., 1: 0.2}\n    X1 = np.random.binomial(1, 0.6)\n    X2 = np.random.binomial(1, 0.5)\n    X3 = np.random.binomial(1, 0.7)\n    ate = ate[arm]\n    mean = 0.35 + ate + 0.3 * X1 + 0.1 * X2 - 0.2 * X3\n    Y_i = generator.binomial(n=1, p=mean)\n    X_df = pd.DataFrame({\"X_1\": [X1], \"X_2\": [X2], \"X_3\": [X3]})\n    return Reward(outcome=float(Y_i), covariates=X_df)\n\ndef reward_no_covariates(arm: int):\n    return Reward(outcome=reward_covariates(arm=arm).outcome)\n\nNext, we will run MAD both with and without covariates. We will see that both approaches give us valid inference, but that, unsurprisingly, including covariates gives us improvements in the precision. We set Thompson Sampling as the underlying adaptive assignment algorithm, a quickly-decaying sequence \\(\\delta_t = \\frac{1}{t^{1/0.24}}\\), and (for MADCovar) a linear probability model for the outcome models fit on \\(\\{X_{1, i}, X_{2, i}, X_{3, i}\\}\\). Finally, we simulate MAD and MADCovar for 2,000 units and calculate the corresponding ATE estimates and 95% CSs.\nFirst, we will run our experiment with MADCovar:\n\nmad_covariates = MAD(\n    bandit=TSBernoulli(k=2, control=0, reward=reward_covariates),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=int(2e3),\n    model=FastOLSModel,\n    pooled=True\n)\nmad_covariates.fit(\n    verbose=False,\n    early_stopping=False,\n    mc_adjust=None\n)\n\nand then repeat the experiment with MAD:\n\nmad_no_covariates = MAD(\n    bandit=TSBernoulli(k=2, control=0, reward=reward_no_covariates),\n    alpha=0.05,\n    delta=lambda x: 1./(x**0.24),\n    t_star=int(2e3)\n)\nmad_no_covariates.fit(verbose=False, early_stopping=False, mc_adjust=None)\n\nNow, we can compare the precision of the 95% CSs for both methods.\n\n\n\n\n\n\n\n\n\nWe see that, as expected, including covariates improves our precision across the entire sample horizon.\nFor further details and examples, please check out the remainder of our documentation!"
  },
  {
    "objectID": "reference/bandit.Reward.html",
    "href": "reference/bandit.Reward.html",
    "title": "bandit.Reward",
    "section": "",
    "text": "bandit.Reward(self, outcome, covariates=None)\nA simple class for reward functions.\nEach reward function should return a reward object. For covariate adjusted algorithms, the reward should contain both the outcome as well as the corresponding covariates. For non-covariate-adjusted algorithms, only the outcome should be specified.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\noutcome\nfloat\nThe outcome of the reward function.\n\n\ncovariates\npd.DataFrame | None\n(Optional) The corresponding individual-level covariates."
  },
  {
    "objectID": "reference/bandit.Reward.html#attributes",
    "href": "reference/bandit.Reward.html#attributes",
    "title": "bandit.Reward",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\noutcome\nfloat\nThe outcome of the reward function.\n\n\ncovariates\npd.DataFrame | None\n(Optional) The corresponding individual-level covariates."
  },
  {
    "objectID": "reference/model.Model.html",
    "href": "reference/model.Model.html",
    "title": "model.Model",
    "section": "",
    "text": "model.Model()\nA model class to support a variety of different models. This should accept an input matrix or dataframe X and response array y. This class must also implement the following methods:\n\n\nThe following methods must be implemented:\nfit() This method should fit the model and return the fitted model object. The class should also record the fitted model under the self._fitted attribute. That way the user can access the fitted model directly or after the fact. predict(X): This method should generate predicted values from the fitted model on a holdout set and return these predictions as a numpy array.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nFit the underlying model and return the model object.\n\n\npredict\nReturns the predictions of the fitted model on a hold-out dataset.\n\n\n\n\n\nmodel.Model.fit()\nFit the underlying model and return the model object.\nThis method should also store the model at the self._fitted attribute for direct user access.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAny\nAn arbitrary fitted model.\n\n\n\n\n\n\n\nmodel.Model.predict(X)\nReturns the predictions of the fitted model on a hold-out dataset.\nThis function should return predictions as a numpy array.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame | np.array | Any\nAny rectangular data structure that can generate predictions using the np.dot(X, beta) procedure.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nA one-dimension numpy array with predicted hold-out values."
  },
  {
    "objectID": "reference/model.Model.html#notes",
    "href": "reference/model.Model.html#notes",
    "title": "model.Model",
    "section": "",
    "text": "The following methods must be implemented:\nfit() This method should fit the model and return the fitted model object. The class should also record the fitted model under the self._fitted attribute. That way the user can access the fitted model directly or after the fact. predict(X): This method should generate predicted values from the fitted model on a holdout set and return these predictions as a numpy array."
  },
  {
    "objectID": "reference/model.Model.html#methods",
    "href": "reference/model.Model.html#methods",
    "title": "model.Model",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfit\nFit the underlying model and return the model object.\n\n\npredict\nReturns the predictions of the fitted model on a hold-out dataset.\n\n\n\n\n\nmodel.Model.fit()\nFit the underlying model and return the model object.\nThis method should also store the model at the self._fitted attribute for direct user access.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAny\nAn arbitrary fitted model.\n\n\n\n\n\n\n\nmodel.Model.predict(X)\nReturns the predictions of the fitted model on a hold-out dataset.\nThis function should return predictions as a numpy array.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame | np.array | Any\nAny rectangular data structure that can generate predictions using the np.dot(X, beta) procedure.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nA one-dimension numpy array with predicted hold-out values."
  },
  {
    "objectID": "reference/MADMod.html",
    "href": "reference/MADMod.html",
    "title": "MADMod",
    "section": "",
    "text": "MADMod(\n    self,\n    bandit,\n    alpha,\n    delta,\n    t_star,\n    decay=lambda x: 1 / np.sqrt(x),\n    model=None,\n    n_warmup=1,\n    pooled=True,\n)\nThis class implements Power-adjusted MAD (MADMod; Molitor and Gold, 2025).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbandit\npyssed.Bandit\nThis object must implement several crucial methods/attributes. For more details on how to create a custom Bandit object, see the documentation of the Bandit class.\nrequired\n\n\nalpha\nfloat\nThe size of the statistical test (testing for non-zero treatment effects)\nrequired\n\n\ndelta\nCallable[[int], float]\nA function that generates the real-valued sequence delta_t in Liang and Bojinov (Definition 4 - Mixture Adaptive Design). This sequence should converge to 0 slower than 1/t^(1/4) where t denotes the time frame in {0, … n}. This function should intake an integer (t) and output a float (the corresponding delta_t)\nrequired\n\n\nt_star\nint\nThe time-step at which we want to optimize the CSs to be tightest. E.g. Liang and Bojinov set this to the max horizon of their experiment.\nrequired\n\n\ndecay\nCallable[[int], float]\ndecay() should intake the current time step t and output a value in [0, 1] where 1 represents no decay and 0 represents complete decay. This function is similar to the delta() argument above. However, delta() determines the amount of random exploration in the MAD algorithm at time t. In contrast, decay() is a time-decreasing function that determines how quickly the bandit assignment probabilities for arm k decay to 0 once arm k’s ATE (ATE_k) is statistically significant. Setting a constant decay = lambda _: 1 makes this method identical to the vanilla MAD design. In contrast, decay = lambda _: 0 is the same as setting the bandit probabilities for arm k to 0 as soon as it has a significant ATE.\nlambda x: 1 / np.sqrt(x)\n\n\nmodel\nModel\n(Optional) The model class used to estimated the outcome models. This is only relevant when using covariate adjustment.\nNone\n\n\nn_warmup\nint\nThe number of units (t) to collect before constructing CSs and estimating the outcome models. This is only relevant when using covariate adjustment.\n1\n\n\npooled\nbool\nWhether the outcome models should be estimated separately for each treatment arm or if a single, pooled model should be estimated. This is only relevant when using covariate adjustment.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n_weights\nDict[int, float]\nThese weights are calculated by decay(). Each arm has a weight. When an arm has a weight of 1, its bandit assignment probabilities are not adjusted. Once an arm is statistically significant, its weight begins to decay towards 0, and so does its bandit assignment probabilities. As an arm’s weight decays, it “shifts” its probability onto currently under-powered arms. This iterative procedure continues to focus more sample on under-powered arms until either all arms have significant ATEs or the experiment has ended.\n\n\n\n\n\n\nSee the documentation for pyssed.mad.MADBase for details on the majority of the provided methods.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npull\nPerform one full iteration of the modified MAD algorithm\n\n\n\n\n\nMADMod.pull(early_stopping=True, cs_precision=0.1, mc_adjust='Bonferroni')\nPerform one full iteration of the modified MAD algorithm"
  },
  {
    "objectID": "reference/MADMod.html#parameters",
    "href": "reference/MADMod.html#parameters",
    "title": "MADMod",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbandit\npyssed.Bandit\nThis object must implement several crucial methods/attributes. For more details on how to create a custom Bandit object, see the documentation of the Bandit class.\nrequired\n\n\nalpha\nfloat\nThe size of the statistical test (testing for non-zero treatment effects)\nrequired\n\n\ndelta\nCallable[[int], float]\nA function that generates the real-valued sequence delta_t in Liang and Bojinov (Definition 4 - Mixture Adaptive Design). This sequence should converge to 0 slower than 1/t^(1/4) where t denotes the time frame in {0, … n}. This function should intake an integer (t) and output a float (the corresponding delta_t)\nrequired\n\n\nt_star\nint\nThe time-step at which we want to optimize the CSs to be tightest. E.g. Liang and Bojinov set this to the max horizon of their experiment.\nrequired\n\n\ndecay\nCallable[[int], float]\ndecay() should intake the current time step t and output a value in [0, 1] where 1 represents no decay and 0 represents complete decay. This function is similar to the delta() argument above. However, delta() determines the amount of random exploration in the MAD algorithm at time t. In contrast, decay() is a time-decreasing function that determines how quickly the bandit assignment probabilities for arm k decay to 0 once arm k’s ATE (ATE_k) is statistically significant. Setting a constant decay = lambda _: 1 makes this method identical to the vanilla MAD design. In contrast, decay = lambda _: 0 is the same as setting the bandit probabilities for arm k to 0 as soon as it has a significant ATE.\nlambda x: 1 / np.sqrt(x)\n\n\nmodel\nModel\n(Optional) The model class used to estimated the outcome models. This is only relevant when using covariate adjustment.\nNone\n\n\nn_warmup\nint\nThe number of units (t) to collect before constructing CSs and estimating the outcome models. This is only relevant when using covariate adjustment.\n1\n\n\npooled\nbool\nWhether the outcome models should be estimated separately for each treatment arm or if a single, pooled model should be estimated. This is only relevant when using covariate adjustment.\nTrue"
  },
  {
    "objectID": "reference/MADMod.html#attributes",
    "href": "reference/MADMod.html#attributes",
    "title": "MADMod",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n_weights\nDict[int, float]\nThese weights are calculated by decay(). Each arm has a weight. When an arm has a weight of 1, its bandit assignment probabilities are not adjusted. Once an arm is statistically significant, its weight begins to decay towards 0, and so does its bandit assignment probabilities. As an arm’s weight decays, it “shifts” its probability onto currently under-powered arms. This iterative procedure continues to focus more sample on under-powered arms until either all arms have significant ATEs or the experiment has ended."
  },
  {
    "objectID": "reference/MADMod.html#notes",
    "href": "reference/MADMod.html#notes",
    "title": "MADMod",
    "section": "",
    "text": "See the documentation for pyssed.mad.MADBase for details on the majority of the provided methods."
  },
  {
    "objectID": "reference/MADMod.html#methods",
    "href": "reference/MADMod.html#methods",
    "title": "MADMod",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npull\nPerform one full iteration of the modified MAD algorithm\n\n\n\n\n\nMADMod.pull(early_stopping=True, cs_precision=0.1, mc_adjust='Bonferroni')\nPerform one full iteration of the modified MAD algorithm"
  },
  {
    "objectID": "reference/MADBase.html",
    "href": "reference/MADBase.html",
    "title": "MADBase",
    "section": "",
    "text": "MADBase(self, bandit, alpha, delta, t_star, model=None, n_warmup=1, pooled=True)\nBase class that includes methods for both the pyssed.MAD and pyssed.MADMod classes.\n\n\n\n\n\nName\nDescription\n\n\n\n\nestimates\nExtract estimated ATEs and confidence sequences.\n\n\nfit\nFit the MAD algorithm for the full time horizon.\n\n\nplot_ate\nPlot the ATE and CSs for each arm at the current time step.\n\n\nplot_ate_path\nPlot the ATE and CS paths for each arm of the experiment.\n\n\nplot_ites\nPlot the estimated individual treatment effects (ITEs).\n\n\nplot_n\nPlot the total N assigned to each arm.\n\n\nplot_probabilities\nPlot the arm assignment probabilities across time.\n\n\nplot_sample\nPlot sample assignment to arms across time\n\n\nsummary\nPrint a summary of ATEs and confidence bands.\n\n\n\n\n\nMADBase.estimates()\nExtract estimated ATEs and confidence sequences.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nA dataframe of ATE estimates and corresponding CS lower and upper bounds.\n\n\n\n\n\n\n\nMADBase.fit(\n    early_stopping=True,\n    cs_precision=0.1,\n    mc_adjust='Bonferroni',\n    verbose=True,\n    **kwargs,\n)\nFit the MAD algorithm for the full time horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nearly_stopping\nbool\nWhether or not to stop the experiment early when all the arms have statistically significant ATEs.\nTrue\n\n\ncs_precision\nfloat\nThis parameter controls how precise we want to make our Confidence Sequences (CSs). If cs_precision = 0 then the experiment will stop immediately as soon as all arms are statistically significant. If cs_precision = 0.2 then the experiment will run until all CSs are at least 20% tighter (shorter) than they were when they became statistically significant. If cs_precision = 0.4 the experiment will run until all CSs are at least 40% tighter, and so on.\n0.1\n\n\nmc_adjust\nstr\nThe type of multiple comparison correction to apply to the constructed CSs. Default is “Bonferroni” (currently “Bonferroni” or None are the only supported options).\n'Bonferroni'\n\n\nverbose\nbool\nWhether to print progress of the algorithm\nTrue\n\n\n**kwargs\nAny\nKeyword arguments to pass directly to the self.pull method. For more details see the documentation for that method.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\nMADBase.plot_ate()\nPlot the ATE and CSs for each arm at the current time step.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplotnine.ggplot\n\n\n\n\n\n\n\n\nMADBase.plot_ate_path()\nPlot the ATE and CS paths for each arm of the experiment.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplotnine.ggplot\n\n\n\n\n\n\n\n\nMADBase.plot_ites(arm, type='boxplot', **kwargs)\nPlot the estimated individual treatment effects (ITEs).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narm\nint\nThe index of the arm for which to plot ITE estimates.\nrequired\n\n\ntype\nstr\nThe type of plot. Must be one of ‘boxplot’, ‘density’, or ‘histogram’.\n'boxplot'\n\n\n**kwargs\n\nKeyword arguments to pass directly to the geom_{plot_type}() call.\n{}\n\n\n\n\n\n\n\nMADBase.plot_n()\nPlot the total N assigned to each arm.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplotnine.ggplot\n\n\n\n\n\n\n\n\nMADBase.plot_probabilities()\nPlot the arm assignment probabilities across time.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplotnine.ggplot\n\n\n\n\n\n\n\n\nMADBase.plot_sample()\nPlot sample assignment to arms across time\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplotnine.ggplot\n\n\n\n\n\n\n\n\nMADBase.summary()\nPrint a summary of ATEs and confidence bands.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone"
  },
  {
    "objectID": "reference/MADBase.html#methods",
    "href": "reference/MADBase.html#methods",
    "title": "MADBase",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nestimates\nExtract estimated ATEs and confidence sequences.\n\n\nfit\nFit the MAD algorithm for the full time horizon.\n\n\nplot_ate\nPlot the ATE and CSs for each arm at the current time step.\n\n\nplot_ate_path\nPlot the ATE and CS paths for each arm of the experiment.\n\n\nplot_ites\nPlot the estimated individual treatment effects (ITEs).\n\n\nplot_n\nPlot the total N assigned to each arm.\n\n\nplot_probabilities\nPlot the arm assignment probabilities across time.\n\n\nplot_sample\nPlot sample assignment to arms across time\n\n\nsummary\nPrint a summary of ATEs and confidence bands.\n\n\n\n\n\nMADBase.estimates()\nExtract estimated ATEs and confidence sequences.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nA dataframe of ATE estimates and corresponding CS lower and upper bounds.\n\n\n\n\n\n\n\nMADBase.fit(\n    early_stopping=True,\n    cs_precision=0.1,\n    mc_adjust='Bonferroni',\n    verbose=True,\n    **kwargs,\n)\nFit the MAD algorithm for the full time horizon.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nearly_stopping\nbool\nWhether or not to stop the experiment early when all the arms have statistically significant ATEs.\nTrue\n\n\ncs_precision\nfloat\nThis parameter controls how precise we want to make our Confidence Sequences (CSs). If cs_precision = 0 then the experiment will stop immediately as soon as all arms are statistically significant. If cs_precision = 0.2 then the experiment will run until all CSs are at least 20% tighter (shorter) than they were when they became statistically significant. If cs_precision = 0.4 the experiment will run until all CSs are at least 40% tighter, and so on.\n0.1\n\n\nmc_adjust\nstr\nThe type of multiple comparison correction to apply to the constructed CSs. Default is “Bonferroni” (currently “Bonferroni” or None are the only supported options).\n'Bonferroni'\n\n\nverbose\nbool\nWhether to print progress of the algorithm\nTrue\n\n\n**kwargs\nAny\nKeyword arguments to pass directly to the self.pull method. For more details see the documentation for that method.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\nMADBase.plot_ate()\nPlot the ATE and CSs for each arm at the current time step.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplotnine.ggplot\n\n\n\n\n\n\n\n\nMADBase.plot_ate_path()\nPlot the ATE and CS paths for each arm of the experiment.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplotnine.ggplot\n\n\n\n\n\n\n\n\nMADBase.plot_ites(arm, type='boxplot', **kwargs)\nPlot the estimated individual treatment effects (ITEs).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narm\nint\nThe index of the arm for which to plot ITE estimates.\nrequired\n\n\ntype\nstr\nThe type of plot. Must be one of ‘boxplot’, ‘density’, or ‘histogram’.\n'boxplot'\n\n\n**kwargs\n\nKeyword arguments to pass directly to the geom_{plot_type}() call.\n{}\n\n\n\n\n\n\n\nMADBase.plot_n()\nPlot the total N assigned to each arm.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplotnine.ggplot\n\n\n\n\n\n\n\n\nMADBase.plot_probabilities()\nPlot the arm assignment probabilities across time.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplotnine.ggplot\n\n\n\n\n\n\n\n\nMADBase.plot_sample()\nPlot sample assignment to arms across time\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nplotnine.ggplot\n\n\n\n\n\n\n\n\nMADBase.summary()\nPrint a summary of ATEs and confidence bands.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone"
  },
  {
    "objectID": "reference/bandit.Bandit.html",
    "href": "reference/bandit.Bandit.html",
    "title": "bandit.Bandit",
    "section": "",
    "text": "bandit.Bandit()\nAn abstract class for Bandit algorithms used in the MAD algorithm.\nEach bandit algorithm that inherits from this class must implement all the abstract methods defined in this class.\n\n\nSee the detailed method documentation for in-depth explanations.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncontrol\nGet the index of the bandit control arm.\n\n\nk\nGet the number of bandit arms.\n\n\nprobabilities\nCalculate bandit arm assignment probabilities.\n\n\nreward\nCalculate the reward for a selected bandit arm.\n\n\nt\nGet the current time step of the bandit.\n\n\n\n\n\nbandit.Bandit.control()\nGet the index of the bandit control arm.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nThe index of the arm that is the control arm. E.g. if the bandit is a 3-arm bandit with the first arm being the control arm, this should return the value 0.\n\n\n\n\n\n\n\nbandit.Bandit.k()\nGet the number of bandit arms.\nint The number of arms in the bandit.\n\n\n\nbandit.Bandit.probabilities()\nCalculate bandit arm assignment probabilities.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[int, float]\nA dictionary where keys are arm indices and values are the corresponding probabilities. For example, if the bandit algorithm is UCB with three arms, and the third arm has the maximum confidence bound, then this should return the following dictionary: {0: 0., 1: 0., 2: 1.}, since UCB is deterministic.\n\n\n\n\n\n\n\nbandit.Bandit.reward(arm)\nCalculate the reward for a selected bandit arm.\nReturns the reward for a selected arm.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narm\nint\nThe index of the selected bandit arm.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nReward\nThe resulting Reward containing any individual-level covariates and the observed reward.\n\n\n\n\n\n\n\nbandit.Bandit.t()\nGet the current time step of the bandit.\nThis method returns the current time step of the bandit, and then increments the time step by 1. E.g. if the bandit has completed 9 iterations, this should return the value 10. Time steps start at 1, not 0.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nThe current time step."
  },
  {
    "objectID": "reference/bandit.Bandit.html#notes",
    "href": "reference/bandit.Bandit.html#notes",
    "title": "bandit.Bandit",
    "section": "",
    "text": "See the detailed method documentation for in-depth explanations."
  },
  {
    "objectID": "reference/bandit.Bandit.html#methods",
    "href": "reference/bandit.Bandit.html#methods",
    "title": "bandit.Bandit",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncontrol\nGet the index of the bandit control arm.\n\n\nk\nGet the number of bandit arms.\n\n\nprobabilities\nCalculate bandit arm assignment probabilities.\n\n\nreward\nCalculate the reward for a selected bandit arm.\n\n\nt\nGet the current time step of the bandit.\n\n\n\n\n\nbandit.Bandit.control()\nGet the index of the bandit control arm.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nThe index of the arm that is the control arm. E.g. if the bandit is a 3-arm bandit with the first arm being the control arm, this should return the value 0.\n\n\n\n\n\n\n\nbandit.Bandit.k()\nGet the number of bandit arms.\nint The number of arms in the bandit.\n\n\n\nbandit.Bandit.probabilities()\nCalculate bandit arm assignment probabilities.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDict[int, float]\nA dictionary where keys are arm indices and values are the corresponding probabilities. For example, if the bandit algorithm is UCB with three arms, and the third arm has the maximum confidence bound, then this should return the following dictionary: {0: 0., 1: 0., 2: 1.}, since UCB is deterministic.\n\n\n\n\n\n\n\nbandit.Bandit.reward(arm)\nCalculate the reward for a selected bandit arm.\nReturns the reward for a selected arm.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\narm\nint\nThe index of the selected bandit arm.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nReward\nThe resulting Reward containing any individual-level covariates and the observed reward.\n\n\n\n\n\n\n\nbandit.Bandit.t()\nGet the current time step of the bandit.\nThis method returns the current time step of the bandit, and then increments the time step by 1. E.g. if the bandit has completed 9 iterations, this should return the value 10. Time steps start at 1, not 0.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nint\nThe current time step."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "Classes and functions that are crucial for both the MAD and MADMod classes\n\n\n\nMADBase\nBase class that includes methods for both the pyssed.MAD and pyssed.MADMod classes.\n\n\nbandit.Bandit\nAn abstract class for Bandit algorithms used in the MAD algorithm.\n\n\nbandit.Reward\nA simple class for reward functions.\n\n\nmodel.Model\nA model class to support a variety of different models. This should accept\n\n\n\n\n\n\nMAD that supports covariate adjustment for improved ATE precision\n\n\n\nMAD\nA class implementing the Mixture-Adaptive Design\n\n\n\n\n\nMAD that dynamically adjusts sample allocation to improve statistical power\n\n\n\nMADMod\nThis class implements Power-adjusted MAD (MADMod; Molitor and Gold, 2025)."
  },
  {
    "objectID": "reference/index.html#foundational-functions",
    "href": "reference/index.html#foundational-functions",
    "title": "Reference",
    "section": "",
    "text": "Classes and functions that are crucial for both the MAD and MADMod classes\n\n\n\nMADBase\nBase class that includes methods for both the pyssed.MAD and pyssed.MADMod classes.\n\n\nbandit.Bandit\nAn abstract class for Bandit algorithms used in the MAD algorithm.\n\n\nbandit.Reward\nA simple class for reward functions.\n\n\nmodel.Model\nA model class to support a variety of different models. This should accept"
  },
  {
    "objectID": "reference/index.html#covariate-adjusted-mad",
    "href": "reference/index.html#covariate-adjusted-mad",
    "title": "Reference",
    "section": "",
    "text": "MAD that supports covariate adjustment for improved ATE precision\n\n\n\nMAD\nA class implementing the Mixture-Adaptive Design\n\n\n\n\n\nMAD that dynamically adjusts sample allocation to improve statistical power\n\n\n\nMADMod\nThis class implements Power-adjusted MAD (MADMod; Molitor and Gold, 2025)."
  },
  {
    "objectID": "reference/MAD.html",
    "href": "reference/MAD.html",
    "title": "MAD",
    "section": "",
    "text": "MAD(self, bandit, alpha, delta, t_star, model=None, n_warmup=1, pooled=True)\nA class implementing the Mixture-Adaptive Design (MAD; Liang and Bojinov, 2024) both with or without covariate adjustment (MADCovar; Molitor and Gold, 2025).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbandit\npyssed.Bandit\nThis object must implement several crucial methods/attributes. For more details on how to create a custom Bandit object, see the documentation of the Bandit class.\nrequired\n\n\nalpha\nfloat\nThe size of the statistical test (testing for non-zero treatment effects)\nrequired\n\n\ndelta\nCallable[[int], float]\nA function that generates the real-valued sequence delta_t in Liang and Bojinov (Definition 4 - Mixture Adaptive Design). This sequence should converge to 0 slower than 1/t^(1/4) where t denotes the time frame in {0, … n}. This function should intake an integer (t) and output a float (the corresponding delta_t)\nrequired\n\n\nt_star\nint\nThe time-step at which we want to optimize the CSs to be tightest. E.g. Liang and Bojinov set this to the max horizon of their experiment.\nrequired\n\n\nmodel\nModel\n(Optional) The model class used to estimated the outcome models. This is only relevant when using covariate adjustment. Please see pyssed.model.Model for more details.\nNone\n\n\nn_warmup\nint\nThe number of units (t) to collect before constructing CSs and estimating the outcome models. This is only relevant when using covariate adjustment.\n1\n\n\npooled\nbool\nWhether the outcome models should be estimated separately for each treatment arm or if a single, pooled model should be estimated. This is only relevant when using covariate adjustment.\nTrue\n\n\n\n\n\n\nSee the documentation for pyssed.mad.MADBase for details on the majority of the provided methods.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npull\nPerform one full iteration of the MAD algorithm.\n\n\n\n\n\nMAD.pull(early_stopping=True, cs_precision=0.1, mc_adjust='Bonferroni')\nPerform one full iteration of the MAD algorithm.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nearly_stopping\nbool\nWhether or not to stop the experiment early when all the arms have statistically significant ATEs.\nTrue\n\n\ncs_precision\nfloat\nThis parameter controls how precise we want to make our Confidence Sequences (CSs). If cs_precision = 0 then the experiment will stop immediately as soon as all arms are statistically significant. If cs_precision = 0.2 then the experiment will run until all CSs are at least 20% tighter (shorter) than they were when they became statistically significant. If cs_precision = 0.4 the experiment will run until all CSs are at least 40% tighter, and so on.\n0.1\n\n\nmc_adjust\nstr\nThe type of multiple comparison correction to apply to the constructed CSs. Default is “Bonferroni” (currently “Bonferroni” or None are the only supported options).\n'Bonferroni'"
  },
  {
    "objectID": "reference/MAD.html#parameters",
    "href": "reference/MAD.html#parameters",
    "title": "MAD",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbandit\npyssed.Bandit\nThis object must implement several crucial methods/attributes. For more details on how to create a custom Bandit object, see the documentation of the Bandit class.\nrequired\n\n\nalpha\nfloat\nThe size of the statistical test (testing for non-zero treatment effects)\nrequired\n\n\ndelta\nCallable[[int], float]\nA function that generates the real-valued sequence delta_t in Liang and Bojinov (Definition 4 - Mixture Adaptive Design). This sequence should converge to 0 slower than 1/t^(1/4) where t denotes the time frame in {0, … n}. This function should intake an integer (t) and output a float (the corresponding delta_t)\nrequired\n\n\nt_star\nint\nThe time-step at which we want to optimize the CSs to be tightest. E.g. Liang and Bojinov set this to the max horizon of their experiment.\nrequired\n\n\nmodel\nModel\n(Optional) The model class used to estimated the outcome models. This is only relevant when using covariate adjustment. Please see pyssed.model.Model for more details.\nNone\n\n\nn_warmup\nint\nThe number of units (t) to collect before constructing CSs and estimating the outcome models. This is only relevant when using covariate adjustment.\n1\n\n\npooled\nbool\nWhether the outcome models should be estimated separately for each treatment arm or if a single, pooled model should be estimated. This is only relevant when using covariate adjustment.\nTrue"
  },
  {
    "objectID": "reference/MAD.html#notes",
    "href": "reference/MAD.html#notes",
    "title": "MAD",
    "section": "",
    "text": "See the documentation for pyssed.mad.MADBase for details on the majority of the provided methods."
  },
  {
    "objectID": "reference/MAD.html#methods",
    "href": "reference/MAD.html#methods",
    "title": "MAD",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npull\nPerform one full iteration of the MAD algorithm.\n\n\n\n\n\nMAD.pull(early_stopping=True, cs_precision=0.1, mc_adjust='Bonferroni')\nPerform one full iteration of the MAD algorithm.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nearly_stopping\nbool\nWhether or not to stop the experiment early when all the arms have statistically significant ATEs.\nTrue\n\n\ncs_precision\nfloat\nThis parameter controls how precise we want to make our Confidence Sequences (CSs). If cs_precision = 0 then the experiment will stop immediately as soon as all arms are statistically significant. If cs_precision = 0.2 then the experiment will run until all CSs are at least 20% tighter (shorter) than they were when they became statistically significant. If cs_precision = 0.4 the experiment will run until all CSs are at least 40% tighter, and so on.\n0.1\n\n\nmc_adjust\nstr\nThe type of multiple comparison correction to apply to the constructed CSs. Default is “Bonferroni” (currently “Bonferroni” or None are the only supported options).\n'Bonferroni'"
  }
]